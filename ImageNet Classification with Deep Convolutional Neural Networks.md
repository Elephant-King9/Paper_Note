# ImageNet Classification with Deep Convolutional Neural Networks

***

ğŸ“…Â å‡ºç‰ˆå¹´ä»½:2012\ ğŸ“–Â å‡ºç‰ˆæœŸåˆŠ:\ ğŸ“ˆÂ å½±å“å› å­:\ ğŸ§‘Â æ–‡ç« ä½œè€…:Krizhevsky Alex,Sutskever Ilya,Hinton Geoffrey E

***

## ğŸ”Â æ‘˜è¦:

We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7% and 18.9% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.

***

## ğŸŒÂ ç ”ç©¶ç›®çš„:

## ğŸ“°Â ç ”ç©¶èƒŒæ™¯:

## ğŸ”¬Â ç ”ç©¶æ–¹æ³•:

***

## ğŸ”©Â æ¨¡å‹æ¶æ„:

â€œThe neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax.â€



***

## ğŸ§ªÂ å®éªŒ:

### Â ğŸ“‡ Â æ•°æ®é›†:

â€œILSVRC-2010 and ILSVRC-2012â€

â€œILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images.â€

â€œGiven a rectangular image, we first rescaled the image such that the shorter side was of length 256, and then cropped out the central 256Ã—256 patch from the resulting image. We did not pre-process the images in any other way, except for subtracting the mean activity over the training set from each pixel. So we trained our network on the (centered) raw RGB values of the pixels.â€ (Krizhevsky ç­‰, 2012, p. 2)

### Â ğŸ§ Â ç¥ç»ç½‘ç»œåˆ›æ–°:

1.â€œReLU Nonlinearityâ€ (Krizhevsky ç­‰, 2012, p. 3)



â€œA four-layer convolutional neural network with ReLUs (solid line) reaches a 25% training error rate on CIFAR-10 six times faster than an equivalent network with tanh neurons (dashed line).â€

2.â€œTraining on Multiple GPUsâ€ (Krizhevsky ç­‰, 2012, p. 3)

â€œTherefore we spread the net across two GPUs. Current GPUs are particularly well-suited to cross-GPU parallelization, as they are able to read from and write to one anotherâ€™s memory directly, without going through host machine memory.â€

3.â€œLocal Response Normalizationâ€



â€œResponse normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. We also verified the effectiveness of this scheme on the CIFAR-10 dataset: a four-layer CNN achieved a 13% test error rate without normalization and 11% with normalization3.â€ (Krizhevsky ç­‰, 2012, p. 4)

4.â€œOverlapping Poolingâ€ (Krizhevsky ç­‰, 2012, p. 4)

â€œThis is what we use throughout our network, with s = 2 and z = 3. This scheme reduces the top-1 and top-5 error rates by 0.4% and 0.3%, respectively,â€

â€œWe generally observe during training that models with overlapping pooling find it slightly more difficult to overfit.â€ (Krizhevsky ç­‰, 2012, p. 4)

### Â ğŸ“‰Â ä¼˜åŒ–å™¨è¶…å‚æ•°:

â€œWe trained our models using stochastic gradient descent with a batch size of 128 examples, momentum of 0.9, and weight decay of 0.0005.â€

â€œWe initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1.â€ â€œWe initialized the neuron biases in the remaining layers with the constant 0.â€ (Krizhevsky ç­‰, 2012, p. 6)

â€œThe heuristic which we followed was to divide the learning rate by 10 when the validation error rate stopped improving with the current learning rate. The learning rate was initialized at 0.01 andâ€ (Krizhevsky ç­‰, 2012, p. 6)

â€œreduced three times prior to termination. We trained the network for roughly 90 cyclesâ€ (Krizhevsky ç­‰, 2012, p. 7)

### Â âš ï¸Â è¿‡æ‹Ÿåˆé—®é¢˜:

1.â€œData Augmentationâ€ (Krizhevsky ç­‰, 2012, p. 5)

â€œThe first form of data augmentation consists of generating image translations and horizontal reflections.â€ (Krizhevsky ç­‰, 2012, p. 5)

â€œThe first form of data augmentation consists of generating image translations and horizontal reflections. We do this by extracting random 224 Ã— 224 patches (and their horizontal reflections) from the 256Ã—256 images and training our network on these extracted patches4â€ (Krizhevsky ç­‰, 2012, p. 5)

â€œAt test time, the network makes a prediction by extracting five 224 Ã— 224 patches (the four corner patches and the center patch) as well as their horizontal reflections (hence ten patches in all), and averaging the predictions made by the networkâ€™s softmax layer on the ten patches.â€ (Krizhevsky ç­‰, 2012, p. 5)

â€œThe second form of data augmentation consists of altering the intensities of the RGB channels in training images.â€ (Krizhevsky ç­‰, 2012, p. 5)

use PCA

â€œThis scheme approximately captures an important property of natural images, namely, that object identity is invariant to changes in the intensity and color of the illumination. This scheme reduces the top-1 error rate by over 1%.â€ (Krizhevsky ç­‰, 2012, p. 6)

2.â€œDropoutâ€ (Krizhevsky ç­‰, 2012, p. 6)

â€œThe neurons which are â€œdropped outâ€ in this way do not contribute to the forward pass and do not participate in backpropagation. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons.â€ (Krizhevsky ç­‰, 2012, p. 6)

â€œAt test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks.â€ (Krizhevsky ç­‰, 2012, p. 6)

### Â ğŸ’» Â å®éªŒè®¾å¤‡:

â€œOur network takes between five and six days to train on two GTX 580 3GB GPUsâ€ (Krizhevsky ç­‰, 2012, p. 2)

### Â ğŸ“Š Â æ¨¡å‹æå‡:

We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.â€

### Â ğŸ“‹ Â å®éªŒç»“æœ:

â€œILSVRC-2010â€ (Krizhevsky ç­‰, 2012, p. 7)



â€œILSVRC-2012â€ (Krizhevsky ç­‰, 2012, p. 7)



### Â ğŸ“ˆ Â å®šæ€§åˆ†æ:

â€œThe kernels on GPU 1 are largely color-agnostic, while the kernels on on GPU 2 are largely color-specific.â€





### Â ğŸ“‰ Â å®šé‡åˆ†æ:

***

## ğŸš©Â ç ”ç©¶ç»“è®º:

***

## ğŸ“Â æ€»ç»“

### ğŸ’¡Â åˆ›æ–°ç‚¹:

æå‡ºäº†ä¸€ç§æ–°çš„ç½‘ç»œAlexNetï¼Œåœ¨ImageNetä¸Šå–å¾—äº†çªç ´æ€§çš„è¿›æ­¥

ä½¿ç”¨äº†æ·±å±‚çš„å·ç§¯ç¥ç»ç½‘ç»œ

*   ReLUä½œä¸ºéçº¿æ€§æ¿€æ´»å‡½æ•°
*   åŒGPUå¹¶è¡Œè®­ç»ƒ
*   ä½¿ç”¨äº†å±€éƒ¨å“åº”å½’ä¸€åŒ–LRN
*   ä½¿ç”¨äº†é‡å æ± åŒ–

åœ¨è¿‡æ‹Ÿåˆçš„é—®é¢˜ä¸Šï¼Œä½¿ç”¨äº†

*   æ•°æ®å¢å¼º

    *   å¯¹å›¾åƒè¿›è¡Œå››è§’æˆ–ä¸­å¿ƒè£å‰ª
    *   å¯¹å›¾åƒæ ¹æ®PCAä¸»æˆåˆ†åˆ†æè¿›è¡Œå˜åŒ–

*   ä½¿ç”¨äº†Dropout

åœ¨å¾—å‡ºçš„ç»“æœä¸­å®šæ€§åˆ†æå¾—çŸ¥ï¼Œç¥ç»ç½‘ç»œæ˜¯çœŸæ­£çš„å­¦ä¹ åˆ°äº†çŸ¥è¯†ï¼Œè€Œä¸æ˜¯ç®€å•çš„åˆ¤æ–­

### Â âš Â å±€é™æ€§:

### Â ğŸ”§Â æ”¹è¿›æ–¹æ³•:

### Â ğŸ–ï¸Â çŸ¥è¯†è¡¥å……:

***

## ğŸ’¬Â è®¨è®º:
